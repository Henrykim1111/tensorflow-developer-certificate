{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Henrykim1111/tensorflow-developer-certificate/blob/main/TF3_New_Beans_(2022_11_%EC%8B%A0%EA%B7%9C%EC%B6%9C%EC%A0%9C%EB%AC%B8%EC%A0%9C)%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################\n",
        "## [중요]\n",
        "## 2023년 6월 1일 기준\n",
        "## Google Colab에 설치된 텐서플로우(TensorFlow) 버전은 2.12 버전입니다.\n",
        "## 시험에 제출 가능한 버전은 2.9.0 버전이기 때문에 버전을 다운그레이드 진행해야 합니다.\n",
        "## 반드시 아래 코드를 실행하여 버전 다운그레이드 후 모델링을 진행하세요.\n",
        "## 시험 볼 때는 현재 이 코드는 지워주셔야 합니다.\n",
        "## 관련하여 궁금하신 점은 슬랙 커뮤니티에 질문 남겨 주세요.\n",
        "####################################################\n",
        "import urllib.request\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/tfcert.py'\n",
        "urllib.request.urlretrieve(url, 'tfcert.py')\n",
        "%run tfcert.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(f'설치 완료 후 TensorFlow 버전: {tf.__version__}')\n",
        "print(f'설치 완료 후 TensorFlow Datasets 버전: {tfds.__version__}')\n",
        "\n",
        "## 시험을 위한 버전 확인 ###########\n",
        "## TensorFlow:          2.9.0 #\n",
        "## TensorFlow Datasets: 4.6.0 #\n",
        "###############################"
      ],
      "metadata": {
        "id": "ra-hof4DK7CJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f24ca98-21fc-4a26-82b4-1349202a7883"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "설치환경: Google Colab\n",
            "TensorFlow 시험환경을 구성중입니다. 잠시만 기다려 주세요.\n",
            "(설치는 약 1~5분 정도 소요 됩니다)\n",
            "============================================================\n",
            "============================================================\n",
            "[알림] TensorFlow 시험환경 구성이 완료 되었습니다.\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "설치 완료 후 TensorFlow 버전: 2.9.0\n",
            "설치 완료 후 TensorFlow Datasets 버전: 4.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hJOExfavKQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f7ecd1-79a5-45c9-cc62-4afbd679806a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "20/33 [=================>............] - ETA: 4:21 - loss: 1.8498 - acc: 0.4531"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative to its\n",
        "# difficulty. So your Category 1 question will score significantly\n",
        "# less than your Category 5 question.\n",
        "#\n",
        "# WARNING: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure. You do not need them to solve the question.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ==============================================================================\n",
        "#\n",
        "# COMPUTER VISION WITH CNNs\n",
        "#\n",
        "# Create and train a classifier to classify images between three categories\n",
        "# of beans using the beans dataset.\n",
        "# ==============================================================================\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# Beans dataset has images belonging to 3 classes as follows:\n",
        "# 2 disease classes (Angular leaf spot, bean rust)\n",
        "# 1 healthy class (healthy).\n",
        "# The images are of different sizes and have 3 channels.\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# We have already divided the data for training and validation.\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. preprocess()\n",
        "# 2. solution_model()\n",
        "#\n",
        "# Your code will fail to be graded if the following criteria are not met:\n",
        "# 1. The input shape of your model must be (300,300,3), because the testing\n",
        "#    infrastructure expects inputs according to this specification. You must\n",
        "#    resize all the images in the dataset to this size while pre-processing\n",
        "#    the dataset.\n",
        "# 2. The last layer of your model must be a Dense layer with 3 neurons\n",
        "#    activated by softmax since this dataset has 3 classes.\n",
        "#\n",
        "# HINT: Your neural network must have a validation accuracy of approximately\n",
        "# 0.75 or above on the normalized validation dataset for top marks.\n",
        "#\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Use this constant wherever necessary\n",
        "IMG_SIZE = 300\n",
        "\n",
        "# This function normalizes and resizes the images.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, size=(300,300))\n",
        "    image /= 255\n",
        "    return image, label\n",
        "\n",
        "\n",
        "# This function loads the data, normalizes and resizes the images, splits it into\n",
        "# train and validation sets, defines the model, compiles it and finally\n",
        "# trains the model. The trained model is returned from this function.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION.\n",
        "def solution_model():\n",
        "    # Loads and splits the data into training and validation splits using tfds.\n",
        "    (ds_train, ds_validation), ds_info = tfds.load(\n",
        "        name='beans',\n",
        "        split=['train', 'validation'],\n",
        "        as_supervised=True,\n",
        "        with_info=True)\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    # Resizes and normalizes train and validation datasets using the\n",
        "    # preprocess() function.\n",
        "    # Also makes other calls, as evident from the code, to prepare them for\n",
        "    # training.\n",
        "    ds_train = ds_train.map(preprocess).cache().shuffle(\n",
        "        ds_info.splits['train'].num_examples).batch(BATCH_SIZE).prefetch(\n",
        "        tf.data.experimental.AUTOTUNE)\n",
        "    ds_validation = ds_validation.map(preprocess).batch(BATCH_SIZE).cache(\n",
        "\n",
        "    ).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    transfer_model = VGG16(weights='imagenet', include_top=False, input_shape=(300, 300, 3))\n",
        "    transfer_model.trainable = False\n",
        "\n",
        "    # Code to define the model\n",
        "    model = tf.keras.models.Sequential([\n",
        "\t\t    transfer_model,\n",
        "\t\t    Flatten(),\n",
        "\t\t    Dropout(0.25),\n",
        "\t\t    Dense(128, activation='relu'),\n",
        "\t\t    Dropout(0.25),\n",
        "\t\t    Dense(32, activation='relu'),\n",
        "\t\t    tf.keras.layers.Dense(3, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    # Code to compile and train the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "    checkpoint_path = \"my_checkpoint.ckpt\"\n",
        "    checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1)\n",
        "\n",
        "    model.fit(ds_train,\n",
        "          validation_data=ds_validation,\n",
        "          epochs=20,\n",
        "          callbacks=[checkpoint],\n",
        "          )\n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ]
    }
  ]
}